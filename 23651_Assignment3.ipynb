{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "q1rNHXSVtFjd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(23651)\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "with open(\"themes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    THEMES = json.load(f)\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "maze_size = (20, 20)\n",
    "participant_id = 23651 ## SR.No\n",
    "enable_enemy = False\n",
    "enable_trap_boost = True\n",
    "save_path = f\"{participant_id}_enabled_1.pkl\"\n",
    "\n",
    "\n",
    "# Q-learning parameters\n",
    "###################################\n",
    "#      WRITE YOUR CODE BELOW      #\n",
    "num_actions = 4\n",
    "gamma =   0.8             # between 0 - 1\n",
    "alpha =        0.1        # between 0 - 1\n",
    "epsilon =      0.9        # between 0 - 1\n",
    "epsilon_decay =   0.99     # between 0.1 - 1\n",
    "min_epsilon = 0.1\n",
    "num_episodes =    10000\n",
    "max_steps =  1000\n",
    "###################################     \n",
    "\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
    "\n",
    "# ========== REWARDS ==========\n",
    "REWARD_GOAL = 2000 # Reward for reaching goal .\n",
    "REWARD_TRAP = -25 # Trap cell .\n",
    "REWARD_OBSTACLE = -2000 # Obstacle cell .\n",
    "REWARD_REVISIT = -500 # Revisiting same cell .\n",
    "REWARD_ENEMY = -50 # Getting caught by enemy .\n",
    "REWARD_STEP = -5 # Per - step time penalty .\n",
    "REWARD_BOOST = 100 # Boost cell .\n",
    "# ######### #########################\n",
    "# =============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "FKJ5KM_rtJ3q"
   },
   "outputs": [],
   "source": [
    "# Environment\n",
    "class MazeGymEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        initialize the maze_size, participant_id, enable_enemy, enable_trap_boost and max_steps variables\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        #      WRITE YOUR CODE BELOW      #\n",
    "        self.maze_size = maze_size\n",
    "        self.participant_id = participant_id\n",
    "        self.enable_enemy = enable_enemy\n",
    "        self.enable_trap_boost = enable_trap_boost\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        ###################################\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(maze_size[0]),\n",
    "            spaces.Discrete(maze_size[1])\n",
    "        ))\n",
    "\n",
    "        \"\"\"\n",
    "        generate  self.maze using the _generate_obstacles method\n",
    "        make self.start as the top left cell of the maze and self.goal as the bottom right\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        #      WRITE YOUR CODE BELOW      #\n",
    "        self.maze = self._generate_obstacles()\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
    "        \n",
    "        ###################################\n",
    "\n",
    "        if self.enable_trap_boost:\n",
    "            self.trap_cells, self.boost_cells = self._generate_traps_and_boosts(self.maze)\n",
    "        else:\n",
    "            self.trap_cells, self.boost_cells = ([], [])\n",
    "\n",
    "        self.enemy_cells = []\n",
    "        self.current_step = 0\n",
    "        self.agent_pos = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _generate_obstacles(self):\n",
    "        \"\"\"\n",
    "        generates the maze with random obstacles based on the SR.No.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.participant_id)\n",
    "        maze = np.zeros(self.maze_size, dtype=int)\n",
    "        mask = np.ones(self.maze_size, dtype=bool)\n",
    "        safe_cells = [\n",
    "            (0, 0), (0, 1), (1, 0),\n",
    "            (self.maze_size[0]-1, self.maze_size[1]-1), (self.maze_size[0]-2, self.maze_size[1]-1),\n",
    "            (self.maze_size[0]-1, self.maze_size[1]-2)\n",
    "        ]\n",
    "        for row, col in safe_cells:\n",
    "            mask[row, col] = False\n",
    "        maze[mask] = np.random.choice([0, 1], size=mask.sum(), p=[0.9, 0.1])\n",
    "        return maze\n",
    "\n",
    "    def _generate_traps_and_boosts(self, maze):\n",
    "        \"\"\"\n",
    "        generates special cells, traps and boosts. While training our agent,\n",
    "        we want to pass thru more number of boost cells and avoid trap cells \n",
    "        \"\"\"\n",
    "        if not self.enable_trap_boost:\n",
    "            return [], []\n",
    "        exclusions = {self.start, self.goal}\n",
    "        empty_cells = list(zip(*np.where(maze == 0)))\n",
    "        valid_cells = [cell for cell in empty_cells if cell not in exclusions]\n",
    "        num_traps = self.maze_size[0] * 2\n",
    "        num_boosts = self.maze_size[0] * 2\n",
    "        random.seed(self.participant_id)\n",
    "        trap_cells = random.sample(valid_cells, num_traps)\n",
    "        trap_cells_ = trap_cells\n",
    "        remaining_cells = [cell for cell in valid_cells if cell not in trap_cells]\n",
    "        boost_cells = random.sample(remaining_cells, num_boosts)\n",
    "        boost_cells_ = boost_cells\n",
    "        return trap_cells, boost_cells\n",
    "\n",
    "    def move_enemy(self, enemy_pos):\n",
    "        possible_moves = []\n",
    "        for dx, dy in actions:\n",
    "            new_pos = (enemy_pos[0] + dx, enemy_pos[1] + dy)\n",
    "            if (0 <= new_pos[0] < self.maze_size[0] and\n",
    "                0 <= new_pos[1] < self.maze_size[1] and\n",
    "                self.maze[new_pos] != 1):\n",
    "                possible_moves.append(new_pos)\n",
    "        return random.choice(possible_moves) if possible_moves else enemy_pos\n",
    "\n",
    "    def update_enemies(self):\n",
    "        if self.enable_enemy:\n",
    "            self.enemy_cells = [self.move_enemy(enemy) for enemy in self.enemy_cells]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        empty_cells = list(zip(*np.where(self.maze == 0)))\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
    "\n",
    "        for pos in (self.start, self.goal):\n",
    "            if pos in self.trap_cells:\n",
    "                self.trap_cells.remove(pos)\n",
    "            if pos in self.boost_cells:\n",
    "                self.boost_cells.remove(pos)\n",
    "\n",
    "        if self.enable_enemy:\n",
    "            enemy_candidates = [cell for cell in empty_cells if cell not in {self.start, self.goal}]\n",
    "            num_enemies = max(1, int((self.maze_size[0] * self.maze_size[1]) / 100))\n",
    "            self.enemy_cells = random.sample(enemy_candidates, min(num_enemies, len(enemy_candidates)))\n",
    "        else:\n",
    "            self.enemy_cells = []\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.agent_pos = self.start\n",
    "        self.visited = set()\n",
    "\n",
    "\n",
    "        return self.agent_pos, {}\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state == self.goal:\n",
    "            return REWARD_GOAL\n",
    "        elif state in self.trap_cells:\n",
    "            return REWARD_TRAP\n",
    "        elif state in self.boost_cells:\n",
    "            return REWARD_BOOST\n",
    "        elif self.maze[state] == 1:\n",
    "            return REWARD_OBSTACLE\n",
    "        else:\n",
    "            return REWARD_STEP\n",
    "\n",
    "    def take_action(self, state, action):\n",
    "        attempted_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        if (0 <= attempted_state[0] < self.maze_size[0] and\n",
    "            0 <= attempted_state[1] < self.maze_size[1] and\n",
    "            self.maze[attempted_state] != 1):\n",
    "            return attempted_state, False\n",
    "        else:\n",
    "            return state, True\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        next_state, wall_collision = self.take_action(self.agent_pos, action)\n",
    "        if wall_collision:\n",
    "            reward = REWARD_OBSTACLE\n",
    "            next_state = self.agent_pos\n",
    "        else:\n",
    "            if self.enable_enemy:\n",
    "                self.update_enemies()\n",
    "            if self.enable_enemy and next_state in self.enemy_cells:\n",
    "                reward = REWARD_ENEMY\n",
    "                done = True\n",
    "                truncated = True\n",
    "                info = {'terminated_by': 'enemy'}\n",
    "                self.agent_pos = next_state\n",
    "                return self.agent_pos, reward, done, truncated, info\n",
    "            else:\n",
    "                revisit_penalty = REWARD_REVISIT if next_state in self.visited else 0\n",
    "                self.visited.add(next_state)\n",
    "                reward = self.get_reward(next_state) + revisit_penalty\n",
    "        self.agent_pos = next_state\n",
    "\n",
    "        if self.agent_pos == self.goal:\n",
    "            done = True\n",
    "            truncated = False\n",
    "            info = {'completed_by': 'goal'}\n",
    "        elif self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            truncated = True\n",
    "            info = {'terminated_by': 'timeout'}\n",
    "        else:\n",
    "            done = False\n",
    "            truncated = False\n",
    "            info = {\n",
    "                'current_step': self.current_step,\n",
    "                'agent_position': self.agent_pos,\n",
    "                'remaining_steps': self.max_steps - self.current_step\n",
    "            }\n",
    "\n",
    "        return self.agent_pos, reward, done, truncated, info\n",
    "\n",
    "    def render(self, path=None, theme=\"racing\"):\n",
    "        icons = THEMES.get(theme, THEMES[\"racing\"])\n",
    "        clear_output(wait=True)\n",
    "        grid = np.full(self.maze_size, icons[\"empty\"])\n",
    "        grid[self.maze == 1] = icons[\"obstacle\"]\n",
    "        for cell in self.trap_cells:\n",
    "            grid[cell] = icons[\"trap\"]\n",
    "        for cell in self.boost_cells:\n",
    "            grid[cell] = icons[\"boost\"]\n",
    "        grid[self.start] = icons[\"start\"]\n",
    "        grid[self.goal] = icons[\"goal\"]\n",
    "        if path is not None:\n",
    "            for cell in path[1:-1]:\n",
    "                if grid[cell] not in (icons[\"goal\"], icons[\"obstacle\"], icons[\"trap\"], icons[\"boost\"]):\n",
    "                    grid[cell] = icons[\"path\"]\n",
    "        if self.agent_pos is not None:\n",
    "            if grid[self.agent_pos] not in (icons[\"goal\"], icons[\"obstacle\"]):\n",
    "                grid[self.agent_pos] = icons[\"agent\"]\n",
    "        if self.enable_enemy:\n",
    "            for enemy in self.enemy_cells:\n",
    "                grid[enemy] = icons[\"enemy\"]\n",
    "        df = pd.DataFrame(grid)\n",
    "        print(df.to_string(index=False, header=False))\n",
    "\n",
    "    def print_final_message(self, success, interrupted, caught, theme):\n",
    "        msgs = THEMES.get(theme, THEMES[\"racing\"]).get(\"final_messages\", {})\n",
    "        if interrupted:\n",
    "            print(f\"\\n{msgs.get('Interrupted', 'ğŸ›‘ Interrupted.')}\")\n",
    "        elif caught:\n",
    "            print(f\"\\n{msgs.get('Defeat', 'ğŸš“ Caught by enemy.')}\")\n",
    "        elif success:\n",
    "            print(f\"\\n{msgs.get('Triumph', 'ğŸ Success.')}\")\n",
    "        else:\n",
    "            print(f\"\\n{msgs.get('TimeOut', 'â›½ Time Out.')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "-En22A4ftMdT"
   },
   "outputs": [],
   "source": [
    "# Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, maze_size, num_actions, alpha=0.1, gamma=0.99):\n",
    "        \"\"\"\n",
    "        initialize self.num_actions, self.alpha, self.gamma\n",
    "        initialize self.q_table based on number of states and number of actions\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        #      WRITE YOUR CODE BELOW      #\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = np.zeros((maze_size[0], maze_size[1], num_actions))\n",
    "\n",
    "        ###################################\n",
    "        \n",
    "\n",
    "    def choose_action(self, env, state, epsilon):\n",
    "        \"\"\"\n",
    "        returns an integer between [0,3]\n",
    "\n",
    "        epsilon is a parameter between 0 and 1.\n",
    "        It is the probability with which we choose an exploratory action (random action)\n",
    "        Eg: ---\n",
    "        If epsilon = 0.25, probability of choosing action from q_table = 0.75\n",
    "                           probability of choosing random action = 0.25\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        #      WRITE YOUR CODE BELOW      #\n",
    "        random_prob = np.random.rand()\n",
    "        if random_prob < epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "            \n",
    "        ###################################\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Use the Q-learning update equation to update the Q-Table\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        #      WRITE YOUR CODE BELOW      #\n",
    "        curr_qval=self.q_table[state[0],state[0],action]\n",
    "        best_next_qval=np.max(self.q_table[next_state[0],next_state[1]])\n",
    "        new_qval= curr_qval + self.alpha*(reward + self.gamma*best_next_qval - curr_qval)\n",
    "\n",
    "        #now we update the q_table\n",
    "        self.q_table[state[0],state[1],action]=new_qval\n",
    "\n",
    "        ###################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ebdQy-8tPw-",
    "outputId": "ac14a863-71c6-47af-f00e-842c43502cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at episode 0: 1000 steps and Reward -1081805.00\n",
      "Episode 0/10000 - Epsilon: 0.9900 - Total Steps: 1000 - Episode Reward: -1081805.00 - Best Reward: -1081805.00\n",
      "New best at episode 2: 1000 steps and Reward -1040195.00\n",
      "New best at episode 3: 1000 steps and Reward -940235.00\n",
      "New best at episode 12: 1000 steps and Reward -925750.00\n",
      "New best at episode 13: 413 steps and Reward -364600.00\n",
      "New best at episode 14: 344 steps and Reward -276370.00\n",
      "New best at episode 17: 330 steps and Reward -271115.00\n",
      "New best at episode 21: 165 steps and Reward -110880.00\n",
      "New best at episode 29: 105 steps and Reward -48240.00\n",
      "New best at episode 75: 93 steps and Reward -32645.00\n",
      "New best at episode 77: 86 steps and Reward -31725.00\n",
      "New best at episode 110: 79 steps and Reward -27870.00\n",
      "New best at episode 115: 67 steps and Reward -11230.00\n",
      "New best at episode 123: 50 steps and Reward -9800.00\n",
      "New best at episode 134: 48 steps and Reward -5955.00\n",
      "New best at episode 150: 44 steps and Reward -1455.00\n",
      "New best at episode 181: 38 steps and Reward 2220.00\n",
      "New best at episode 282: 44 steps and Reward 2420.00\n",
      "New best at episode 287: 46 steps and Reward 2430.00\n",
      "New best at episode 982: 62 steps and Reward 2690.00\n",
      "Episode 1000/10000 - Epsilon: 0.1000 - Total Steps: 156 - Episode Reward: -249745.00 - Best Reward: 2690.00\n",
      "Episode 2000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2219510.00 - Best Reward: 2690.00\n",
      "Episode 3000/10000 - Epsilon: 0.1000 - Total Steps: 158 - Episode Reward: -246635.00 - Best Reward: 2690.00\n",
      "Episode 4000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2239685.00 - Best Reward: 2690.00\n",
      "Episode 5000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2280765.00 - Best Reward: 2690.00\n",
      "Episode 6000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2262605.00 - Best Reward: 2690.00\n",
      "Episode 7000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2233925.00 - Best Reward: 2690.00\n",
      "Episode 8000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2232390.00 - Best Reward: 2690.00\n",
      "Episode 9000/10000 - Epsilon: 0.1000 - Total Steps: 1000 - Episode Reward: -2227005.00 - Best Reward: 2690.00\n",
      "\n",
      "Training completed. Total episodes: 9999\n",
      "Hence, Number of Steps taken by the agent: 62\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
    "agent = QLearningAgent(maze_size, num_actions)\n",
    "\n",
    "start_episode = 0\n",
    "episode=start_episode\n",
    "best_reward = -np.inf\n",
    "best_q_table = None\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(\"Checkpoint found. Loading...\")\n",
    "    with open(save_path, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "        agent.q_table = checkpoint['q_table']\n",
    "        start_episode = checkpoint['episode']\n",
    "        epsilon = checkpoint['epsilon']\n",
    "        best_q_table = checkpoint.get('best_q_table', agent.q_table.copy())\n",
    "        best_reward = checkpoint.get('best_reward', -np.inf)\n",
    "        best_step_counter = checkpoint.get('best_step_counter', 0)\n",
    "    print(f\"Resuming from episode {start_episode} with epsilon {epsilon:.4f}, best reward {best_reward:.2f} and best step {best_step_counter}\")\n",
    "else:\n",
    "    epsilon = 1.0\n",
    "\n",
    "try:\n",
    "    for episode in range(start_episode, num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        visited_states = set()\n",
    "        episode_reward = 0\n",
    "        step_counter = 0\n",
    "        # episode_path=[]#To track path for rendering maze :)\n",
    "        # episode_path.append(state)\n",
    "        while not done and step_counter < max_steps:\n",
    "            action = agent.choose_action(env, state, epsilon)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if next_state in visited_states:\n",
    "                reward += REWARD_REVISIT\n",
    "            visited_states.add(next_state)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_counter += 1\n",
    "            # episode_path.append(state)  #add current updated state\n",
    "\n",
    "            if state == env.goal:\n",
    "                done = True\n",
    "\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_q_table = agent.q_table.copy()\n",
    "            best_step_counter = step_counter\n",
    "            # best_path = episode_path.copy() #Track best path\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'q_table': agent.q_table,\n",
    "                    'episode': episode,\n",
    "                    'epsilon': epsilon,\n",
    "                    'best_q_table': best_q_table,\n",
    "                    'best_reward': best_reward,\n",
    "                    'best_step_counter': best_step_counter,\n",
    "                    # 'best_path': best_path\n",
    "                }, f)\n",
    "            print(f\"New best at episode {episode}: {step_counter} steps and Reward {best_reward:.2f}\")\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes} - Epsilon: {epsilon:.4f} - Total Steps: {step_counter} - Episode Reward: {episode_reward:.2f} - Best Reward: {best_reward:.2f}\")\n",
    "\n",
    "    # best_path.append(state)        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted.\")\n",
    "    print(f\"Interrupted at episode {episode} with epsilon: {epsilon:.4f}, Total Steps: {step_counter}, Episode Reward: {episode_reward:.2f}\")\n",
    "else:\n",
    "    print(f\"\\nTraining completed. Total episodes: {episode}\")\n",
    "print(\"Hence, Number of Steps taken by the agent:\",best_step_counter )\n",
    "# print(\"length of best path (including pos)\",len(best_path)) #Including start and goal\n",
    "# print(\"Goal:\", goal)\n",
    "# print(\"Second last:\", best_path[-2])\n",
    "# print(\"Best Q-Table[5][0]:\")\n",
    "# print(best_q_table[5][0])\n",
    "\n",
    "#Visualizing the path taken by the agent\n",
    "# render_maze(maze, path=best_path, start=start, goal=goal, theme=theme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "bVahczp5tZTY"
   },
   "outputs": [],
   "source": [
    "def test_agent(env, agent, animated, delay, theme):\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    state = obs\n",
    "    path = [state]\n",
    "    visited_states = set()\n",
    "    total_reward = 0\n",
    "    reward_breakdown = {\n",
    "        'goal':     {'count': 0, 'reward': 0.0},\n",
    "        'trap':     {'count': 0, 'reward': 0.0},\n",
    "        'boost':    {'count': 0, 'reward': 0.0},\n",
    "        'obstacle': {'count': 0, 'reward': 0.0},\n",
    "        'step':     {'count': 0, 'reward': 0.0},\n",
    "        'revisit':  {'count': 0, 'reward': 0.0}\n",
    "    }\n",
    "    caught_by_enemy = False\n",
    "    success = False\n",
    "    interrupted = False\n",
    "\n",
    "    try:\n",
    "        for step in range(env.max_steps):\n",
    "            visited_states.add(state)\n",
    "\n",
    "            action = agent.choose_action(env, state, epsilon=0.0)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if info.get('terminated_by') == 'enemy':\n",
    "                caught_by_enemy = True\n",
    "                reward_breakdown.setdefault('enemy', {'count': 0, 'reward': 0.0})\n",
    "                reward_breakdown['enemy']['count'] += 1\n",
    "                reward_breakdown['enemy']['reward'] += reward\n",
    "                total_reward += reward\n",
    "                path.append(next_state)\n",
    "                break\n",
    "            else:\n",
    "                if (next_state == state) and (reward == REWARD_OBSTACLE):\n",
    "                    reward_breakdown['obstacle']['count'] += 1\n",
    "                    reward_breakdown['obstacle']['reward'] += REWARD_OBSTACLE\n",
    "                elif next_state == env.goal:\n",
    "                    reward_breakdown['goal']['count'] += 1\n",
    "                    reward_breakdown['goal']['reward'] += REWARD_GOAL\n",
    "                elif next_state in env.trap_cells:\n",
    "                    reward_breakdown['trap']['count'] += 1\n",
    "                    reward_breakdown['trap']['reward'] += REWARD_TRAP\n",
    "                elif next_state in env.boost_cells:\n",
    "                    reward_breakdown['boost']['count'] += 1\n",
    "                    reward_breakdown['boost']['reward'] += REWARD_BOOST\n",
    "                elif next_state in visited_states:\n",
    "                    reward += REWARD_REVISIT\n",
    "                    reward_breakdown['revisit']['count'] += 1\n",
    "                    reward_breakdown['revisit']['reward'] += REWARD_REVISIT\n",
    "                reward_breakdown['step']['count'] += 1\n",
    "                reward_breakdown['step']['reward'] += REWARD_STEP\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            path.append(state)\n",
    "\n",
    "            if animated:\n",
    "                env.render(path, theme)\n",
    "                print(f\"\\nTotal Allowed Ateps: {env.max_steps}\")\n",
    "                print(f\"Current Reward: {total_reward:.2f}\")\n",
    "                print(\"Live Reward Breakdown:\")\n",
    "                df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
    "                print(df)\n",
    "                time.sleep(delay)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        interrupted = True\n",
    "\n",
    "    if state == env.goal:\n",
    "        success = True\n",
    "\n",
    "    return path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NkA-fGy4EAS",
    "outputId": "a0658c4e-39a0-4fe6-ac6c-f449d295ea6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found. Loading best Q-table for testing...\n",
      "Best Q-table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
    "agent = QLearningAgent(maze_size, num_actions)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(\"Checkpoint found. Loading best Q-table for testing...\")\n",
    "    with open(save_path, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "        best_q_table = checkpoint.get('best_q_table', checkpoint['q_table'])\n",
    "        agent.q_table = best_q_table\n",
    "    print(\"Best Q-table loaded successfully.\")\n",
    "else:\n",
    "    print(\"No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqEw69hYsmtA",
    "outputId": "af0971ad-cfba-496e-de79-fb3e10fc1051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ´ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ ğŸ’£ ğŸ’£ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ â¬œ â¬œ\n",
      "ğŸª™ ğŸ— â¬œ â¬œ â¬œ ğŸ— ğŸ’£ ğŸª¨ â¬œ ğŸª¨ â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ â¬œ ğŸ’£ ğŸ— â¬œ\n",
      "ğŸ— ğŸ— ğŸª™ ğŸ— ğŸª™ ğŸª™ ğŸª™ ğŸ’£ ğŸª™ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª¨ â¬œ ğŸª¨ ğŸª¨\n",
      "ğŸ’£ â¬œ ğŸ— â¬œ â¬œ â¬œ ğŸª¨ ğŸ— ğŸª™ ğŸ’£ â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ ğŸª¨ ğŸ— â¬œ â¬œ\n",
      "ğŸ’£ â¬œ â¬œ â¬œ â¬œ ğŸª¨ â¬œ â¬œ ğŸ— ğŸ— â¬œ â¬œ ğŸª¨ â¬œ ğŸ— â¬œ â¬œ ğŸ’£ â¬œ ğŸ—\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ ğŸ’£ ğŸ— ğŸ— â¬œ ğŸ’£ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ—\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ â¬œ ğŸª™ ğŸª¨ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ â¬œ â¬œ ğŸª¨\n",
      "ğŸ’£ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ â¬œ â¬œ ğŸª™ ğŸ— â¬œ ğŸ’£ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª¨\n",
      "â¬œ â¬œ ğŸ— â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª™ ğŸª™ â¬œ â¬œ â¬œ â¬œ ğŸ’£ ğŸª¨ â¬œ â¬œ â¬œ\n",
      "â¬œ â¬œ ğŸª¨ ğŸ— â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ ğŸª™ ğŸª¨ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ\n",
      "ğŸ’£ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ— â¬œ ğŸª™ â¬œ â¬œ â¬œ ğŸ— â¬œ â¬œ â¬œ â¬œ ğŸ’£\n",
      "â¬œ â¬œ ğŸ— â¬œ â¬œ ğŸª¨ â¬œ ğŸ— ğŸª™ ğŸ’£ ğŸª™ ğŸª¨ â¬œ ğŸ’£ â¬œ â¬œ ğŸª¨ ğŸ— ğŸª¨ ğŸ’£\n",
      "ğŸ— â¬œ â¬œ ğŸª¨ â¬œ â¬œ ğŸª¨ ğŸ— ğŸª¨ ğŸ’£ â¬œ ğŸ’£ ğŸª¨ ğŸª¨ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸ— ğŸª¨ â¬œ â¬œ â¬œ ğŸ’£ â¬œ â¬œ ğŸ—\n",
      "â¬œ â¬œ â¬œ ğŸª¨ ğŸ— â¬œ â¬œ ğŸ’£ â¬œ â¬œ ğŸª™ ğŸª™ â¬œ â¬œ ğŸ— ğŸ’£ â¬œ â¬œ ğŸª¨ ğŸ’£\n",
      "â¬œ â¬œ â¬œ ğŸª¨ ğŸ— ğŸª™ ğŸª™ ğŸ’£ ğŸª™ ğŸª™ ğŸ’£ â¬œ â¬œ â¬œ ğŸ’£ â¬œ ğŸ’£ ğŸª¨ â¬œ â¬œ\n",
      "â¬œ â¬œ â¬œ â¬œ ğŸª™ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª¨ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸ’£ â¬œ â¬œ â¬œ\n",
      "â¬œ ğŸª¨ â¬œ â¬œ ğŸª™ â¬œ â¬œ â¬œ ğŸª¨ â¬œ â¬œ ğŸ— â¬œ â¬œ ğŸª¨ ğŸ’£ â¬œ ğŸ’£ â¬œ â¬œ\n",
      "â¬œ ğŸ— â¬œ ğŸª¨ ğŸª™ ğŸ— ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸ’£ ğŸ’£ ğŸª™ ğŸª™ â¬œ â¬œ\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ ğŸª¨ â¬œ â¬œ ğŸª¨ ğŸª™ ğŸª™ ğŸª¨ â¬œ ğŸª¨ ğŸ’£ ğŸ— ğŸª™ ğŸª™ ğŸ’°\n",
      "\n",
      "ğŸ’° Treasure secured! You sailed to fortune!\n",
      "         Count  Reward\n",
      "Goal         1  2000.0\n",
      "Trap         8  -200.0\n",
      "Boost       11  1100.0\n",
      "Obstacle     0     0.0\n",
      "Step        62  -310.0\n",
      "Revisit      0     0.0\n",
      "Total           2590.0\n",
      "\n",
      "Total Allowed Ateps: 1000\n"
     ]
    }
   ],
   "source": [
    "# Run test.\n",
    "\n",
    "theme = random.choice(list(THEMES.keys()))\n",
    "plot_delay = 0.1  # Adjust delay as needed\n",
    "\n",
    "path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy = test_agent(env, agent, animated=True, delay=plot_delay, theme=theme)\n",
    "\n",
    "env.render(path, theme=theme)\n",
    "env.print_final_message(success, interrupted, caught=caught_by_enemy, theme=theme)\n",
    "\n",
    "reward_df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
    "reward_df.index = reward_df.index.str.title()\n",
    "reward_df = reward_df.rename(columns={'count': 'Count', 'reward': 'Reward'})\n",
    "total_row = pd.DataFrame({\n",
    "    'Count': [''],\n",
    "    'Reward': [reward_df['Reward'].sum()]\n",
    "}, index=['Total'])\n",
    "reward_df = pd.concat([reward_df, total_row])\n",
    "\n",
    "print(reward_df)\n",
    "print(f\"\\nTotal Allowed Ateps: {max_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename and Submit this file as **SRNO(5digit)_Assignment3.ipynb**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
